[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bits n Bio Compendium",
    "section": "",
    "text": "The Bits in Bio (BiB) Compendium is an ongoing educational initiative that provides clear, accessible explanations of key bioinformatics and techbio topics. It‚Äôs designed for ‚Äúbio-curious‚Äù or ‚Äútech-curious‚Äù professionals‚Äîpeople with technical or biological backgrounds who want to understand the opposite side of the techbio spectrum."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "contribute.html",
    "href": "contribute.html",
    "title": "Contribute",
    "section": "",
    "text": "The Bits in Bio Compendium is a down-to-earth knowledge base for anyone curious about the overlap between biology and technology. It‚Äôs a cross between a wiki and a curated guide, with two types of posts: Articles and Nodes\n\nArticles cover application-level topics (e.g.¬†What are the basic steps of a machine learning pipeline?).\nNodes break down foundational concepts (e.g.¬†What is PCA and dimensionality reduction?).\n\nTogether, these pieces link to each other like a web of knowledge, and point outwards to papers, open-source projects, and other resources so readers can dive deeper.\nWe‚Äôre building this collaboratively, and there are many ways you can contribute ‚Äî no matter your background.\n\nWays to Get Involved\n\n‚úçÔ∏è Write (no expertise required)\nWriters don‚Äôt need to be an expert to write a draft! In fact, many of our best pieces start with someone curious about a topic and willing to distill it for others.\nYou start with a bullet-point outline, get early feedback, then expand into a short draft (about 400‚Äì600 words). Then, after reviews, edit the piece into publish ready material.\n\n\nüîç Review and Edit\nReviewers help refine the draft, add diagrams or references, and ensure clarity. Our review process is lightweight: read through drafts, suggest edits, and point out where explanations might be unclear to newcomers. You‚Äôd spots gaps, confusing phrasing, or link examples.\nThis role is especially valuable for people with cross-disciplinary backgrounds.\n\n\nüå± Share Expertise\nIf you‚Äôre an expert in a specific area, we‚Äôd love to use you as a resource!\nThat could mean answering a quick question in Slack, pointing us to a must-read paper, or clarifying a nuance in a workflow.\nWant to contribute? If you haven‚Äôt already, join the Bits n Bio slack - link on our main website!"
  },
  {
    "objectID": "articles/introduction_to_ml.html",
    "href": "articles/introduction_to_ml.html",
    "title": "Intro",
    "section": "",
    "text": "Machine Learning (ML) is a powerful branch of Artificial Intelligence (AI) that enables computers to learn from data and make predictions or decisions without being explicitly programmed. It‚Äôs like a school for machines‚Äîtraining them to recognize patterns in data and apply that knowledge to new, unseen information.\nA common example is email spam detection: a sample of emails is labeled as ‚Äòspam‚Äô or ‚Äònot spam‚Äô and an ML model is trained to classify new emails based on patterns it learns from the training data.\nBefore diving into the details of ML, let‚Äôs look at the broader structure:\n\nArtificial Intelligence (AI) is the overarching field focused on building systems that can perform tasks requiring human-like intelligence.\nMachine Learning (ML) is a subset of AI that uses algorithms to learn from data.\nDeep Learning (DL) is a further subset of ML that uses multi-layered neural networks to model complex data like images, language, and speech.\n\n\n\nTypes of Machine Learning\n\nSupervised Learning: The model learns from labeled data, where both inputs and expected outputs are known.\n\nExample: Predicting whether a tumor is benign or malignant based on medical imaging data.\n\nUnsupervised Learning: The model finds patterns in unlabeled data.\n\nExample: Clustering gene expression data to identify subtypes of diseases without prior labels.\n\nSemi-Supervised Learning: The model learns from a small amount of labeled data combined with a large amount of unlabeled data‚Äîuseful when labeling is resource-expensive ($), computationally expensive, or generally time-consuming.\n\nExample: Classifying rare diseases using a limited set of labeled patient records along with a large pool of unlabeled cases.\n\n\n\n\nUse in Biology:\nIn biology, data is abundant and complex‚Äîranging from genomics, transcriptomics, proteomics, metabolomics, to medical imaging, electronic health records, and clinical trial data. Machine learning helps navigate this complexity by detecting patterns, and automating data analysis at scale‚Äîenabling discoveries that would be impossible with traditional computational/statistical methods alone.\nML applications in biotech include:\n\nGenomics: Identifying disease-associated variants, predicting gene function, or personalizing therapies based on genetic profiles.\n\nGenomeML\n\nDrug Discovery: Identifying hits from high-throughput compound screens (thousands to millions of compounds), designing novel proteins, predicting drug-target interactions, and optimizing lead compounds.\n\nkMOL - Github repo\nElix Inc.\nhttps://www.nature.com/articles/s41587-024-02127-0\n\nMedical Imaging: Diagnosing diseases, including cancer, from radiology or pathology images using computer vision models.\n\nMedical Open Network for Artificial Intelligence (MONAI)\n\nSynthetic Biology: Designing optimized DNA sequences or metabolic pathways with novel functions, using generative ML models.\n\nhttps://www.nature.com/articles/s41467-022-32661-x?fromPaywallRec=false\n\nEpidemiology: Forecasting disease outbreaks or tracking pandemic trends from real-time data.\n\nEpiLearn\n\nProcess Optimization: Tools for writing better code, or tracking and querying data across multiple experiments.\n\nBenchling\n\n\nA landmark example is AlphaFold: Given just a protein sequence, it predicts its 3D structure with near-experimental accuracy. Trained on databases of known protein structures, AlphaFold has learned the underlying chemical and spatial principles of protein folding‚Äîrevolutionizing structural biology, accelerating drug discovery, and opening new paths in enzyme design, synthetic biology, and disease understanding.\nRead more:\n\nIntroducing GenoML ‚Äî the future of genomic machine learning | CARD\nhttps://www.nature.com/articles/s41592-024-02359-7\nhttps://www.nature.com/articles/s41580-021-00407-0\n\n\n\nFun Facts about ML:\n\nYour phone uses ML every day: for autocorrect, face unlock, voice assistants, and photo sorting.\nML can generate fake pets, and art: GANs (Generative Adversarial Networks) can create realistic images of animals that don‚Äôt exist.\nFeedback can lead to improvement: ML algorithms can learn via trial and error, with feedback helping them become better\nML creates art: tools like DALL¬∑E and DeepDream generate paintings, music, and surreal images.\n\n\n\nIs It Always Perfect?\nNot quite. Machine Learning heavily depends on the quality and diversity of the data it‚Äôs trained on‚Äîand it‚Äôs far from flawless. Some examples:\n\nMedical diagnosis: ML can miss rare conditions or make biased predictions if trained on unbalanced datasets, like those based on one/few ancestries or a single gender.\nSelf-driving cars: Struggle with unexpected situations like unusual weather, odd road signs, and unpredictable pedestrians ‚Äì including in other countries!\nDog or muffin?: Image classifiers have famously confused chihuahuas with blueberry muffins (in their defense, it‚Äôs tricky).\n\n\nP.S. Good news‚Äîmachines still need us. Our jobs are safe‚Ä¶ for now\nRead more:\n\nRising to the challenge of bias in health care AI - PMC"
  },
  {
    "objectID": "articles/ml_in_drug_biomarker_discovery.html",
    "href": "articles/ml_in_drug_biomarker_discovery.html",
    "title": "ML in Drug & Biomarker Discovery: The Pipeline",
    "section": "",
    "text": "Since Machine Learning uses algorithms to learn from data, it can discover new medicines or repurpose existing drugs.\nPredictive modeling is one of the core superpowers of ML. Instead of testing thousands of compounds blindly, ML algorithms can forecast biological activity based on chemical structure and drug-drug interactions based on readily available datasets from patients and pharmaceutical companies.\nMachine learning algorithms improve with more data, like athletes with practice. As these algorithms are trained on data, they produce models: mathematical representations that capture patterns and relationships. ML models are only as good as the data it‚Äôs trained on, requiring extensive preparation before actual drug discovery begins.\nBefore you get started with ML, you can‚Äôt skip this crucial step: Defining your question.\nAre you:\n\nDiscovering drug targets?\nPredicting compound behavior?\nIdentifying disease biomarkers?\n\nEach path needs different data and approaches.\nDon‚Äôt jump into complex algorithms without considering whether basic statistics or lab work might actually be better‚Äîespecially for small rare disease datasets. Conduct a literature review to see what has already been done.\nBuilding an ML model that predicts a patient‚Äôs reaction to a drug requires a lot of consecutive steps. This is called a ‚Äúpipeline‚Äù, a series of processes that transforms raw, messy biological data into trained models.\nBelow is a typical ML pipeline:\n\nAccessing and collecting data: gathering raw information from sources like electronic health records or public databases.\nSelecting the model(s) (algorithms): choosing the right algorithm(s) for the tas.\nPre-processing the data: cleaning and organizing it. This can include handling missing values, scaling numbers, and splitting the dataset for training and testing.\nTraining then validating: the algorithm learns from the data, adjusting itself to recognize useful patterns. Then we validate it on a test data set.\nEvaluating: measures how well the model performs on new data using other metrics like ROC-AUC or F1-score.\nDeployment, understanding, and maintenance: the model is deployed in clinical or research environments, and continuously monitored and updated (similar to software updates).\n\nIn the context of drug and biomarker discovery, this pipeline powers everything from early compound screening to precision diagnostics.\nRead more:\n\nMachine Learning for Drug Discovery\nAI vs Machine Learning\nApplications of Machine Learning for Drug Discovery\n\n\n1. Accessing and collecting data\nAn ML model is only as good as the data it‚Äôs trained on. Access to high quality data is the first major hurdle before any computing takes place. It starts with clearly defining the problem you want to solve, because that helps you decide what data you need.\nFor example, if you‚Äôre building a model to detect pneumonia from chest X-rays, you‚Äôll need a large number of labeled medical images. On the other hand, if you‚Äôre making a product recommendation engine, you might rely on user behavior data like clicks and purchases. Depending on the project, you might also collect data through surveys, sensors, APIs, or web scraping.\nOnce you know what you‚Äôre looking for, you have to figure out where you can get it.\nNot all data is free or easily available as some sources require special permission or payment. Accessing data internal to your company or institute is a great place to start. You can also browse open databases through Google Dataset Search, datasets and models on Hugging Face, or other institutes with open access initiatives, like the Allen Institute, GEO, and ENCODE.\nDepending on where the data is stored, you‚Äôll have to download it directly or work via a remote server (which might take extra steps and credentials). Techbio companies who create proprietary models as their primary product usually have connections to hospitals or other facilities where they can have access to quality patient data\nWhile you‚Äôre collecting your data, keep its structure in mind. It will heavily inform how you use the data to produce the model.\nData comes in all shapes and sizes, and usually falls into one of two categories: Structured and Unstructured.\n\nStructured data, like tables and CSV files, is organized and easy to work with\nUnstructured data, like text documents, photos, or audio, often requires extra processing steps to use in downstream models.\n\nRead more:\n\nIBM: Structured vs Unstructured Data\nAmazon AWS: When to use Structured vs Unstructured Data\nGithub: List of publicly available datasets\n\n\n\n2. Model Selection: Choosing the Right Tool\nWe‚Äôve emphasized how crucial data preprocessing is ‚Äì so why select your model first?\nYour choice depends on the type of data, the biological question, and the amount of training data available.\nModel selection comes first because different algorithms need different data formats. Classification models need binary labels like ‚Äúyes‚Äù/‚Äùno‚Äù so they can be mapped to numeric values (like 0 and 1), while transformer models need raw text broken down into smaller pieces (tokenized) and then converted to embeddings. There‚Äôs no one-size-fits-all algorithm, and if you process your data before choosing your model, chances are you‚Äôll create a formatting nightmare and need to start over.\nSome algorithms/models used in drug discovery can include:\n\nRandom Forests‚Äù Predicting drug function from drug-target interactions and chemical structures.\nSupport Vector Machines (SVMs): Drug classification and pharmaceutical data analysis2.\nConvoluted Neural Networks (CNNs): Predicting drug function based on chemical structure3.\nGraph Neural Networks (GNNs): Used to model molecular structures and drug-target networks.\n\nReferences\n\nRandom-forest model for drug‚Äìtarget interaction prediction via Kullback‚ÄìLeibler divergence | Journal of Cheminformatics | Full Text\nDrug design by machine learning: support vector machines for pharmaceutical data analysis\nLearning Drug Functions from Chemical Structures with Convolutional Neural Networks and Random Forests | Journal of Chemical Information and Modeling\n\n\n\n3. Pre-processing the data\nNow that you‚Äôve selected your model, it‚Äôs time to format your data to match what your algorithm expects.\nReal-world biological data comes with serious challenges:\n\nMissing values in Electronic Health Records (EHRs)\nNoisy gene expression profiles\nBatch effects in multi-omics data\n\nMachine learning models are picky. They can‚Äôt handle NaNs (missing values), inconsistent units, or irrelevant noise - for example, background signals from an assay plate, or metadata like experiment dates that accidentally correlate with outcomes.\nRemember how we said classification models need binary labels converted to numbers, and transformer models need the text to be tokenized? The conversion happens during the preprocessing step.\nCommon techniques:\n\nImputation: Filling in missing values using statistical methods (mean/mode) or predictive models like knearest neighbors.\nNormalization/Scaling: Ensuring data ranges (e.g., gene expression levels) are comparable across samples.\nDimensionality Reduction: Techniques like PCA, UMAP, or t-SNE are often used to simplify high-dimensional datasets‚Äîlike RNA-seq‚Äîwithout losing biological signal.\n\nRead more:\n\nFilling the Gaps: A Comparative Guide to Imputation Techniques in Machine Learning - MachineLearningMastery.com\nData Normalization Explained: Types, Examples, & Methods | Estuary\n\n\n\n4. Training and validating the model\nTraining is where your algorithm transforms from a blank slate into a pattern-recognition powerhouse. Think of it like teaching a medical student to diagnose diseases‚Äîyou show them thousands of cases until they can recognize the subtle signs that distinguish pneumonia from bronchitis.\nDuring training, the algorithm adjusts its internal parameters (weights and biases) to minimize prediction errors. For drug discovery, this might mean learning which molecular features correlate with toxicity, or which gene expression patterns predict drug resistance.\nKey considerations during training:\n\nOverfitting vs.¬†Underfitting: Your model might memorize the training data (overfitting) or fail to capture important patterns (underfitting). It‚Äôs like a student who either memorizes answers without understanding concepts, or one who doesn‚Äôt study enough to pass the exam.\nCross-validation: Split your data into training, validation, and test sets. Never let your model see the test data until final evaluation‚Äîthat‚Äôs cheating!\nHyperparameter tuning: Fine-tune settings like learning rate, batch size, or tree depth. Tools like Optuna or Weights & Biases can automate this process.\n\nTraining challenges in drug discovery:\n\nImbalanced datasets: Most compounds aren‚Äôt drugs, so your ‚Äúactive‚Äù class might be tiny compared to ‚Äúinactive‚Äù compounds\nMulti-target effects: Drugs often hit multiple targets, making simple binary classification insufficient\nTime-series effects: Drug metabolism changes over time, requiring temporal modeling approaches\nIn vitro vs.¬†in vivo translation: Cell-based or biochemical assays can mislead‚Äîstrong activity in a test tube doesn‚Äôt guarantee good behavior in a living organism.\n\nThe training process can take anywhere from minutes (for simple models on small datasets) to weeks (for deep learning models on massive compound libraries). GPU clusters and cloud computing platforms like AWS, Google Cloud, or specialized platforms like Paperspace have made this more accessible to smaller research groups.\nRead more:\n\nMachine Learning Mastery: Overfitting and Underfitting\nWeights & Biases: Hyperparameter Optimization\nDeepChem: Open Source Library for Drug Discovery\nIn Vitro and In Vivo Assessment of ADME and PK Properties During Lead Selection and Lead Optimization ‚Äì Guidelines, Benchmarks and Rules of Thumb\n\n\n\n5. Evaluating: Measuring Success (And Failure)\nSo how do we know if the model actually works?\nWe evaluate (or validate) a model by using a validation dataset, where the data is split into training vs validation data.\n\nTraining data: used to fit the model‚Äîthis is where the algorithm ‚Äúlearns‚Äù patterns\nValidation data: used during model development to tune parameters and compare different models\nTest (or external) data: a completely separate dataset used at the end to measure true performance. If the model does well here, it‚Äôs more likely to generalize to new, real-world data.\n\nEvaluation goes beyond simple accuracy‚Äîespecially in drug discovery where false positives can waste millions of dollars and false negatives might miss life-saving treatments.\nCommon evaluation metrics:\n\nROC-AUC: Measures how well your model distinguishes between active and inactive compounds across different thresholds\nPrecision: Of all compounds your model predicts as ‚Äúhits,‚Äù how many actually are? This metric is critical when experimental validation is expensive.\nRecall (Sensitivity): Of all true hits, how many did your model catch? Important for not missing promising candidates\nF1-score: Balances precision and recall, useful when classes are imbalanced\nMatthews Correlation Coefficient (MCC): Often preferred for highly imbalanced datasets common in drug screening\n\nDomain-specific considerations: For drug discovery, you might also evaluate:\n\nChemical diversity: Does your model find diverse scaffolds or only variations of known drugs?\nADMET properties: Can your model predict Absorption, Distribution, Metabolism, Excretion, and Toxicity?\nSynthetic accessibility: Can the predicted compounds be synthesized??\n\nThe validation reality check: Your model might score 95% accuracy on paper but fail miserably in the wet lab. This is why prospective validation‚Äîtesting predictions on truly new compounds‚Äîis the gold standard. Companies like Atomwise and Benevolent AI have published cases where their ML predictions led to successful experimental validation.\nCommon pitfalls:\n\nData leakage: When future information accidentally creeps into training data\nBatch effects: Your model learns to distinguish between different experimental runs rather than biological signals\nDataset shift: Your training data doesn‚Äôt represent the real-world scenario you‚Äôre trying to solve\n\nRead more:\n\nTraining vs evaluation vs test data sets\nScikit-learn: Model Evaluation Guide\nJournal of Chemical Information and Modeling: Best Practices for QSAR\nNature Reviews Drug Discovery: AI in Drug Discovery Validation\n\n\n\n6. Deployment, Understanding, and Maintenance: From Lab to Real World\nCongratulations! Your model works. Now comes the arguably harder part: making it useful in the real world. Deployment isn‚Äôt just about putting your model on a server‚Äîit‚Äôs about integrating it into existing workflows, making it interpretable to scientists, and keeping it updated as new data arrives.\nDeployment strategies:\n\nAPI endpoints: Allow other researchers or software to query your model programmatically\nWeb applications: User-friendly interfaces for non-technical scientists to upload compounds and get predictions\nIntegration with existing platforms: Many companies integrate ML models directly into their compound management systems or electronic lab notebooks\n\nMaking models interpretable: Black-box predictions aren‚Äôt enough in drug discovery‚Äîscientists need to understand why a compound was flagged as promising. Interpretability techniques include:\n\nSHAP (SHapley Additive exPlanations): Shows which molecular features contributed most to each prediction\nLIME (Local Interpretable Model-agnostic Explanations): Explains individual predictions by perturbing input features\nAttention maps: For neural networks, visualize which parts of a molecule the model ‚Äúfocused on‚Äù\nChemical space visualization: Plot compounds in 2D/3D space to understand model decision boundaries\n\nContinuous monitoring and updates: ML models in drug discovery need constant care:\n\nPerformance monitoring: Track prediction accuracy on new experimental results\nData drift detection: Chemical libraries and assay protocols change over time\nModel retraining: Incorporate new experimental data to improve predictions\nVersion control: Keep track of model versions and their performance metrics\n\nReal-world success stories:\n\nRecursion Pharmaceuticals: Uses ML to analyze cellular imaging data and has multiple compounds in clinical trials\nInsitro: Combines ML with experimental biology, recently partnered with Roche for $3 billion\nExscientia: First AI-designed drug to enter human clinical trials (completed Phase I in 2021)\n\nEthical considerations:\n\nAlgorithmic bias: Ensure models work across different populations and don‚Äôt perpetuate healthcare disparities\nIntellectual property: How do you patent AI-discovered compounds?\nRegulatory approval: FDA and EMA are still developing guidelines for AI-assisted drug development\n\nRead more:\n\nMLOps: Machine Learning Operations\nSHAP Documentation: Model Interpretability\nNature Reviews Drug Discovery: Regulatory Considerations for AI\nRecursion Pharmaceuticals: Case Studies"
  },
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "All Articles",
    "section": "",
    "text": "ML in Drug & Biomarker Discovery: The Pipeline\n\n\n\nAnu Kumar and Simran Chhabria, reviewed by Leo Zhu and Lexi Bounds\n\n\n\n\n\n\n\n\n\n\n\nIntro\n\n\n\nSimran Chhabria, reviewed by Leo Zhu\n\n\n\n\n\n\nNo matching items"
  }
]